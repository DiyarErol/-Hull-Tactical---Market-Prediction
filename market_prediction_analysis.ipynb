{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db1e75",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column groups\n",
    "date_col = 'date_id'\n",
    "target_col = 'market_forward_excess_returns'\n",
    "\n",
    "# Determine feature groups\n",
    "feature_cols = [col for col in train_df.columns if col not in [date_col, target_col]]\n",
    "\n",
    "# Group by prefixes\n",
    "feature_groups = {}\n",
    "for col in feature_cols:\n",
    "    prefix = col.split('_')[0] if '_' in col else 'other'\n",
    "    if prefix not in feature_groups:\n",
    "        feature_groups[prefix] = []\n",
    "    feature_groups[prefix].append(col)\n",
    "\n",
    "print(\"Feature Groups:\")\n",
    "for prefix, cols in sorted(feature_groups.items()):\n",
    "    print(f\"  {prefix}: {len(cols)} features\")\n",
    "\n",
    "print(f\"\\nTotal feature count: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Target Variable Statistics:\")\n",
    "print(train_df[target_col].describe())\n",
    "\n",
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(train_df[target_col], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Target Distribution')\n",
    "axes[0].set_xlabel('market_forward_excess_returns')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].boxplot(train_df[target_col])\n",
    "axes[1].set_title('Target Boxplot')\n",
    "axes[1].set_ylabel('market_forward_excess_returns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values check\n",
    "missing_counts = train_df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(train_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts[missing_counts > 0],\n",
    "    'Missing_Pct': missing_pct[missing_counts > 0]\n",
    "}).sort_values('Missing_Pct', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(f\"{len(missing_df)} columns contain missing values:\")\n",
    "    print(missing_df.head(10))\n",
    "else:\n",
    "    print(\"✓ No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdbf79",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering — Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df, feature_cols, windows=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Add technical indicators:\n",
    "    - RSI (Relative Strength Index)\n",
    "    - MACD (Moving Average Convergence Divergence)\n",
    "    - EMA (Exponential Moving Average)\n",
    "    - Bollinger Bands\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Take group-wise mean of features\n",
    "    for prefix in ['D', 'E', 'I', 'M', 'P', 'S', 'V']:\n",
    "        group_cols = [col for col in feature_cols if col.startswith(prefix + '_')]\n",
    "        if len(group_cols) > 0:\n",
    "            group_mean = df_copy[group_cols].mean(axis=1)\n",
    "            \n",
    "            for window in windows:\n",
    "                # Rolling Mean & Std\n",
    "                df_copy[f'{prefix}_rolling_mean_{window}'] = group_mean.rolling(window=window, min_periods=1).mean()\n",
    "                df_copy[f'{prefix}_rolling_std_{window}'] = group_mean.rolling(window=window, min_periods=1).std()\n",
    "                \n",
    "                # EMA\n",
    "                df_copy[f'{prefix}_ema_{window}'] = group_mean.ewm(span=window, adjust=False).mean()\n",
    "            \n",
    "            # RSI (14 period)\n",
    "            delta = group_mean.diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()\n",
    "            rs = gain / (loss + 1e-10)\n",
    "            df_copy[f'{prefix}_rsi_14'] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # MACD\n",
    "            ema12 = group_mean.ewm(span=12, adjust=False).mean()\n",
    "            ema26 = group_mean.ewm(span=26, adjust=False).mean()\n",
    "            df_copy[f'{prefix}_macd'] = ema12 - ema26\n",
    "            df_copy[f'{prefix}_macd_signal'] = df_copy[f'{prefix}_macd'].ewm(span=9, adjust=False).mean()\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            rolling_mean_20 = group_mean.rolling(window=20, min_periods=1).mean()\n",
    "            rolling_std_20 = group_mean.rolling(window=20, min_periods=1).std()\n",
    "            df_copy[f'{prefix}_bb_upper'] = rolling_mean_20 + (2 * rolling_std_20)\n",
    "            df_copy[f'{prefix}_bb_lower'] = rolling_mean_20 - (2 * rolling_std_20)\n",
    "            df_copy[f'{prefix}_bb_width'] = df_copy[f'{prefix}_bb_upper'] - df_copy[f'{prefix}_bb_lower']\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "print(\"Feature engineering function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add technical indicators\n",
    "train_enhanced = add_technical_indicators(train_df, feature_cols)\n",
    "test_enhanced = add_technical_indicators(test_df, feature_cols)\n",
    "\n",
    "print(f\"Original train shape: {train_df.shape}\")\n",
    "print(f\"Enhanced train shape: {train_enhanced.shape}\")\n",
    "print(f\"Number of added features: {train_enhanced.shape[1] - train_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444003c",
   "metadata": {},
   "source": [
    "## 3. Data Preparation & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X_train = train_enhanced.drop(columns=[date_col, target_col])\n",
    "y_train = train_enhanced[target_col]\n",
    "X_test = test_enhanced.drop(columns=[date_col])\n",
    "\n",
    "# Fill missing values\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())  # use train mean for test\n",
    "\n",
    "# Clean infinite values\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.mean())\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_train.mean())\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc046ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation sets with TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    print(f\"Fold {fold}: Train={len(train_idx)}, Val={len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98188ea8",
   "metadata": {},
   "source": [
    "## 4. Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e639c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute model performance metrics\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Direction Accuracy\n",
    "    direction_true = np.sign(y_true)\n",
    "    direction_pred = np.sign(y_pred)\n",
    "    direction_acc = np.mean(direction_true == direction_pred)\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'Direction_Accuracy': direction_acc\n",
    "    }\n",
    "\n",
    "print(\"Metric function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler (leakage-safe)\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Use last fold for validation\n",
    "train_idx, val_idx = list(tscv.split(X_train))[-1]\n",
    "\n",
    "X_tr = X_train.iloc[train_idx]\n",
    "y_tr = y_train.iloc[train_idx]\n",
    "X_val = X_train.iloc[val_idx]\n",
    "y_val = y_train.iloc[val_idx]\n",
    "\n",
    "# Scale\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_tr_scaled.shape}, Val: {X_val_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456347d8",
   "metadata": {},
   "source": [
    "### 4.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88178cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge model\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "# Predictions\n",
    "ridge_train_pred = ridge_model.predict(X_tr_scaled)\n",
    "ridge_val_pred = ridge_model.predict(X_val_scaled)\n",
    "\n",
    "# Metrics\n",
    "ridge_train_metrics = calculate_metrics(y_tr, ridge_train_pred)\n",
    "ridge_val_metrics = calculate_metrics(y_val, ridge_val_pred)\n",
    "\n",
    "print(\"Ridge - Train Metrics:\")\n",
    "for k, v in ridge_train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nRidge - Val Metrics:\")\n",
    "for k, v in ridge_val_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6bea5",
   "metadata": {},
   "source": [
    "### 4.2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_tr_scaled, y_tr)\n",
    "lgb_val = lgb.Dataset(X_val_scaled, y_val, reference=lgb_train)\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    lgb_train,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'val'],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "lgb_train_pred = lgb_model.predict(X_tr_scaled, num_iteration=lgb_model.best_iteration)\n",
    "lgb_val_pred = lgb_model.predict(X_val_scaled, num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "# Metrics\n",
    "lgb_train_metrics = calculate_metrics(y_tr, lgb_train_pred)\n",
    "lgb_val_metrics = calculate_metrics(y_val, lgb_val_pred)\n",
    "\n",
    "print(\"\\nLightGBM - Train Metrics:\")\n",
    "for k, v in lgb_train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nLightGBM - Val Metrics:\")\n",
    "for k, v in lgb_val_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f69fdf",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb829f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "xgb_train = xgb.DMatrix(X_tr_scaled, label=y_tr)\n",
    "xgb_val = xgb.DMatrix(X_val_scaled, label=y_val)\n",
    "\n",
    "evals = [(xgb_train, 'train'), (xgb_val, 'val')]\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    xgb_train,\n",
    "    num_boost_round=500,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "xgb_train_pred = xgb_model.predict(xgb_train, iteration_range=(0, xgb_model.best_iteration))\n",
    "xgb_val_pred = xgb_model.predict(xgb_val, iteration_range=(0, xgb_model.best_iteration))\n",
    "\n",
    "# Metrics\n",
    "xgb_train_metrics = calculate_metrics(y_tr, xgb_train_pred)\n",
    "xgb_val_metrics = calculate_metrics(y_val, xgb_val_pred)\n",
    "\n",
    "print(\"\\nXGBoost - Train Metrics:\")\n",
    "for k, v in xgb_train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nXGBoost - Val Metrics:\")\n",
    "for k, v in xgb_val_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99085997",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4373b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Ridge', 'LightGBM', 'XGBoost'],\n",
    "    'Train_RMSE': [ridge_train_metrics['RMSE'], lgb_train_metrics['RMSE'], xgb_train_metrics['RMSE']],\n",
    "    'Val_RMSE': [ridge_val_metrics['RMSE'], lgb_val_metrics['RMSE'], xgb_val_metrics['RMSE']],\n",
    "    'Train_DirAcc': [ridge_train_metrics['Direction_Accuracy'], lgb_train_metrics['Direction_Accuracy'], xgb_train_metrics['Direction_Accuracy']],\n",
    "    'Val_DirAcc': [ridge_val_metrics['Direction_Accuracy'], lgb_val_metrics['Direction_Accuracy'], xgb_val_metrics['Direction_Accuracy']],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e126083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Görselleştirme\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE karşılaştırması\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, results_df['Train_RMSE'], width, label='Train', alpha=0.8)\n",
    "axes[0].bar(x + width/2, results_df['Val_RMSE'], width, label='Val', alpha=0.8)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(results_df['Model'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Direction Accuracy karşılaştırması\n",
    "axes[1].bar(x - width/2, results_df['Train_DirAcc'], width, label='Train', alpha=0.8)\n",
    "axes[1].bar(x + width/2, results_df['Val_DirAcc'], width, label='Val', alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Direction Accuracy')\n",
    "axes[1].set_title('Direction Accuracy Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(results_df['Model'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fcf34",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': lgb_model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "top_features = importance_df.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Top 20 Features - LightGBM')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "print(top_features.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5fc72f",
   "metadata": {},
   "source": [
    "## 7. SHAP Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values (with sampling)\n",
    "sample_size = min(500, len(X_val_scaled))\n",
    "sample_idx = np.random.choice(len(X_val_scaled), sample_size, replace=False)\n",
    "X_sample = X_val_scaled[sample_idx]\n",
    "\n",
    "explainer = shap.TreeExplainer(lgb_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"SHAP values computed: {shap_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=X_train.columns, max_display=20, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d905af",
   "metadata": {},
   "source": [
    "## 8. Ensemble & Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04653431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions (retrain with full train data)\n",
    "X_full_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Ridge\n",
    "ridge_full = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_full.fit(X_full_scaled, y_train)\n",
    "ridge_test_pred = ridge_full.predict(X_test_scaled)\n",
    "\n",
    "# LightGBM\n",
    "lgb_full_train = lgb.Dataset(X_full_scaled, y_train)\n",
    "lgb_full = lgb.train(lgb_params, lgb_full_train, num_boost_round=lgb_model.best_iteration)\n",
    "lgb_test_pred = lgb_full.predict(X_test_scaled)\n",
    "\n",
    "# XGBoost\n",
    "xgb_full_train = xgb.DMatrix(X_full_scaled, label=y_train)\n",
    "xgb_full = xgb.train(xgb_params, xgb_full_train, num_boost_round=xgb_model.best_iteration)\n",
    "xgb_test = xgb.DMatrix(X_test_scaled)\n",
    "xgb_test_pred = xgb_full.predict(xgb_test)\n",
    "\n",
    "print(\"Test predictions completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble (Weighted Average) — weight by validation RMSE\n",
    "weights = {\n",
    "    'ridge': 1.0 / ridge_val_metrics['RMSE'],\n",
    "    'lgb': 1.0 / lgb_val_metrics['RMSE'],\n",
    "    'xgb': 1.0 / xgb_val_metrics['RMSE']\n",
    "}\n",
    "\n",
    "total_weight = sum(weights.values())\n",
    "weights = {k: v/total_weight for k, v in weights.items()}\n",
    "\n",
    "print(\"Ensemble Weights:\")\n",
    "for model, weight in weights.items():\n",
    "    print(f\"  {model}: {weight:.4f}\")\n",
    "\n",
    "# Ensemble prediction\n",
    "ensemble_pred = (\n",
    "    weights['ridge'] * ridge_test_pred +\n",
    "    weights['lgb'] * lgb_test_pred +\n",
    "    weights['xgb'] * xgb_test_pred\n",
    ")\n",
    "\n",
    "print(f\"\\nEnsemble prediction range: [{ensemble_pred.min():.4f}, {ensemble_pred.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_enhanced[date_col],\n",
    "    'prediction': ensemble_pred\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"✓ submission.csv created\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc3204",
   "metadata": {},
   "source": [
    "## 9. Prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tahminlerinin dağılımı\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Ridge\n",
    "axes[0, 0].hist(ridge_test_pred, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title(f'Ridge Predictions (μ={ridge_test_pred.mean():.4f}, σ={ridge_test_pred.std():.4f})')\n",
    "axes[0, 0].set_xlabel('Prediction')\n",
    "\n",
    "# LightGBM\n",
    "axes[0, 1].hist(lgb_test_pred, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title(f'LightGBM Predictions (μ={lgb_test_pred.mean():.4f}, σ={lgb_test_pred.std():.4f})')\n",
    "axes[0, 1].set_xlabel('Prediction')\n",
    "\n",
    "# XGBoost\n",
    "axes[1, 0].hist(xgb_test_pred, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_title(f'XGBoost Predictions (μ={xgb_test_pred.mean():.4f}, σ={xgb_test_pred.std():.4f})')\n",
    "axes[1, 0].set_xlabel('Prediction')\n",
    "\n",
    "# Ensemble\n",
    "axes[1, 1].hist(ensemble_pred, bins=50, edgecolor='black', alpha=0.7, color='red')\n",
    "axes[1, 1].set_title(f'Ensemble Predictions (μ={ensemble_pred.mean():.4f}, σ={ensemble_pred.std():.4f})')\n",
    "axes[1, 1].set_xlabel('Prediction')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0d3eb",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Completed Steps:\n",
    "1. ✓ Data loading and exploration\n",
    "2. ✓ Feature engineering (RSI, MACD, EMA, Bollinger Bands)\n",
    "3. ✓ Model training (Ridge, LightGBM, XGBoost)\n",
    "4. ✓ Model comparison and metrics\n",
    "5. ✓ Feature importance (gain + SHAP)\n",
    "6. ✓ Ensemble modeling\n",
    "7. ✓ Test submission creation\n",
    "\n",
    "### Next Steps:\n",
    "- Hyperparameter tuning (Optuna)\n",
    "- Robust evaluation with cross-validation\n",
    "- Stacking ensemble\n",
    "- Backtest and risk metrics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
